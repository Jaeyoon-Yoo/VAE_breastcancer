{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "pd.set_option('mode.chained_assignment',  None)\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TF_in_data_all.pickle', 'rb') as f:\n",
    "    TF_in_data, TF_in_std, TF_in_mask, ID_list, ID_year_list = pickle.load(f)\n",
    "TF_in_data = TF_in_data.detach().cpu()\n",
    "TF_in_std = TF_in_std.detach().cpu()\n",
    "TF_in_mask = TF_in_mask.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dir = './loss'\n",
    "model_dir = './model_save'\n",
    "table_loc_master = \"/data/Storage_DAS02/jaeyoon/230101_BreastCancer_Project/data/230629_BC_new/\"\n",
    "table_data_master = \"230913_BT_Timeseries_new.xlsx\"\n",
    "\n",
    "Table_1 = pd.read_excel(table_loc_master+table_data_master).reset_index(drop = True)\n",
    "# Table_Date = Table_1[[\"Test_date_normed\"]]\n",
    "Table_Y_date = Table_1[[\"Date\"]]\n",
    "Table_Y_date.index = [Table_1[\"new_ID\"],Table_1[\"count_in_TF\"]]\n",
    "Table_Y_date[\"Date\"] = [datetime.datetime.strptime(i, '%Y-%m-%d').date() for i in Table_Y_date[\"Date\"]]\n",
    "Table_Y_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_test = pd.read_excel(\"Test_ID_set.xlsx\").values[:,1]\n",
    "ID_train = pd.read_excel(\"Train_ID_set.xlsx\").values[:,1]\n",
    "# ID_test = np.unique([i.split(',')[0][2:-1] for i in ID_test])\n",
    "# ID_train = np.unique([i.split(',')[0][2:-1] for i in ID_train])\n",
    "len(ID_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"/data/Storage_DAS02/jaeyoon/230101_BreastCancer_Project/data/230629_BC_new\"\n",
    "data_recur = \"Recur_new_format_all.xlsx\"\n",
    "Table_Y = pd.read_excel(os.path.join(data_loc,data_recur)).set_index(['new_ID'])\n",
    "Table_Recur = pd.DataFrame(columns = [\"Op_date\", \"Recur_OX\", \"last_record\", \"record_before_recur\", \"recur_date\"])\n",
    "for ID in np.unique(Table_Y.index):\n",
    "    if ID in np.append(ID_test,ID_train):\n",
    "        try:\n",
    "            Recur_OX = Table_Y.loc[ID][\"Recur\"].values[0] != pd.Timestamp('2100-01-01 00:00:00')\n",
    "        except:\n",
    "            Recur_OX = Table_Y.loc[ID][\"Recur\"] != pd.Timestamp('2100-01-01 00:00:00')\n",
    "        try:\n",
    "            Op_date = Table_Y.loc[ID][\"Op\"].values[0]\n",
    "            last_record = Table_Y.loc[ID][\"Date\"].values[-1]\n",
    "        except:\n",
    "            Op_date = Table_Y.loc[ID][\"Op\"]\n",
    "            last_record = Table_Y.loc[ID][\"Date\"]\n",
    "        \n",
    "        if Recur_OX == 0:\n",
    "            Table_Recur.loc[ID] = [Op_date, Recur_OX, last_record, pd.Timestamp('2100-01-01 00:00:00'), pd.Timestamp('2100-01-01 00:00:00')]\n",
    "        elif Recur_OX == 1:\n",
    "            try:\n",
    "                Recur_date = Table_Y.loc[ID][\"Recur\"].values[0]\n",
    "            except:\n",
    "                Recur_date = Table_Y.loc[ID][\"Recur\"]\n",
    "            record_before_recur = Table_Y.loc[ID][\"Date\"].values[Table_Y.loc[ID][\"Date\"].values<Recur_date][-1]\n",
    "            Table_Recur.loc[ID] = [Op_date, Recur_OX, last_record, record_before_recur, Recur_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_Recur_now = pd.DataFrame(columns= [i for i in range(10)])\n",
    "ii = 0\n",
    "for ID in np.append(ID_test,ID_train):\n",
    "    Table_Recur_now.loc[ID] = np.nan\n",
    "    if (ID in Table_Y_date.index) & (ID in Table_Recur.index):\n",
    "        ii+=1\n",
    "        Recur_info = Table_Recur.loc[ID]\n",
    "        \n",
    "        if Recur_info[\"Recur_OX\"]:\n",
    "            Table_Recur_now.loc[ID][[ii for ii in range(min(10,max(1,max([1]+[j+1 for i,j in zip(Table_Y_date.loc[ID].values,Table_Y_date.loc[ID].index) if (i <= Recur_info['record_before_recur'].date())&(type(j)!=str) ])+1)))]] = 0\n",
    "            Table_Recur_now.loc[ID][[ii for ii in range(min(9,min([9]+[j+1 for i,j in zip(Table_Y_date.loc[ID].values,Table_Y_date.loc[ID].index) if (i > Recur_info['recur_date'].date())&(type(j)!=str) ])-1),10)]] = 1\n",
    "        else:\n",
    "            Table_Recur_now.loc[ID][[ii for ii in range(min(10,max(1,max([1]+[j+1 for i,j in zip(Table_Y_date.loc[ID].values,Table_Y_date.loc[ID].index) if (i <= Recur_info['last_record'].date())&(type(j)!=str) ])+1)))]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drug = \"230903_Table_Drug.xlsx\"\n",
    "Table_D = pd.read_excel(os.path.join(data_loc,data_drug)).set_index(['ID','year'])\n",
    "Table_D = (Table_D>0)+0\n",
    "Table_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "class BART_predict_Dataset(Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        TF_in_data, TF_in_mask, ID_list, ID_year_list, Table_Recur_sub, Table_D, ID_use = inputs\n",
    "        idx_sub = [n for n,i in enumerate(ID_list) if i in ID_use]\n",
    "        ID_use = [i for n,i in enumerate(ID_list) if i in ID_use]\n",
    "        self.data = TF_in_data[idx_sub] # N, 10, 50\n",
    "        self.mask = TF_in_mask[idx_sub] # N, 10, 1\n",
    "        self.ID_list = np.array(ID_list)[idx_sub]\n",
    "        self.ID_year_list = [i for n,i in enumerate(ID_year_list) if n in idx_sub]\n",
    "        \n",
    "        self.Recur_mask_TF = torch.from_numpy(Table_Recur_sub.loc[ID_use].notna().values) # N, 10\n",
    "        self.Recur_data_TF = torch.from_numpy(Table_Recur_sub.loc[ID_use].fillna(0).values) # N, 10\n",
    "        \n",
    "        self.D_data_TF = torch.zeros((TF_in_data.shape[0],10,Table_D.shape[1]))\n",
    "        D_idx = Table_D.index.get_level_values('ID').unique()\n",
    "        for n,ID in enumerate(ID_use):\n",
    "            if ID in D_idx:\n",
    "                D_sub = Table_D.loc[ID]\n",
    "                for y_idx in D_sub.index:\n",
    "                    self.D_data_TF[n,y_idx,:] = torch.from_numpy(D_sub.loc[y_idx].values.astype(float))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ID_list)\n",
    "\n",
    "    def __getitem__(self, idx_sub): # N x 260x512\n",
    "        # 260 x 512\n",
    "        return {'data' : self.data[idx_sub],\n",
    "                'mask' : self.mask[idx_sub],\n",
    "                'idx' : self.ID_list[idx_sub],\n",
    "                'Recur_data' : self.Recur_data_TF[idx_sub],\n",
    "                'Recur_mask' : self.Recur_mask_TF[idx_sub],\n",
    "                'Drug_data' : self.D_data_TF[idx_sub],\n",
    "                'data_idx' : self.ID_year_list[idx_sub]}\n",
    "        \n",
    " \n",
    "class BART_sampler():\n",
    "    def __init__(self, rand_p = 0.05):\n",
    "        self.rand_p = rand_p\n",
    "    def BART_rand_fn(self, samples):\n",
    "        # x1,x2,m1,m2,op_data = zip(*batch)\n",
    "        # r_mask_ratio = self.rand_p\n",
    "        # eps = 1e-6\n",
    "        \n",
    "        # x1 = torch.stack(x1).float()\n",
    "        # x2 = torch.stack(x2).float()\n",
    "        # m1 = torch.stack(m1).bool()\n",
    "        # m2 = torch.stack(m2).bool()\n",
    "        # op_data = torch.stack(op_data).float()\n",
    "        \n",
    "        p = self.rand_p\n",
    "        data = torch.stack([sample['data'] for sample in samples]) # N, 10, 50\n",
    "        mask = torch.stack([sample['mask'] for sample in samples]).bool() # N, 10, 1\n",
    "        Recur_data = torch.stack([sample['Recur_data'] for sample in samples]) # N, 10\n",
    "        Recur_mask = torch.stack([sample['Recur_mask'] for sample in samples]).bool() # N, 10 (value_O -> 1)\n",
    "        Recur_data = torch.concat([torch.zeros(Recur_data.shape[0],1), Recur_data],axis = -1) # -> N, 11\n",
    "        Recur_mask = torch.concat([torch.ones(Recur_mask.shape[0],1), Recur_mask],axis = -1).bool() # -> N, 11\n",
    "        Drug_data = torch.stack([sample['Drug_data'] for sample in samples])\n",
    "        IDs = [sample['idx'] for sample in samples] # N, 10, 1\n",
    "        Rand_mask = torch.zeros_like(mask)\n",
    "        if p > 0:\n",
    "            for n,sample in enumerate(samples):\n",
    "                idx_sub = np.setdiff1d(sample['data_idx'],[1])\n",
    "                if idx_sub.shape[0] != 0:\n",
    "                    idx_MLM = np.random.choice(idx_sub,max(1,int(len(idx_sub) * p)),replace = False)            \n",
    "                    for idx in idx_MLM:\n",
    "                        Rand_mask[n,idx] = 1\n",
    "        Rand_mask = Rand_mask.bool()\n",
    "        Enc_mask = mask + Rand_mask\n",
    "        # Enc_mask = mask\n",
    "        Enc_in = data * (~Enc_mask)\n",
    "        Dec_in = torch.concat([torch.zeros(data.shape[0],1,data.shape[2]).float(),Enc_in[:,:-1,:]], axis = 1)\n",
    "        Dec_mask = torch.concat([torch.zeros(data.shape[0],1,1),Enc_mask[:,:-1,:]], axis = 1).repeat(1,1,10).bool()\n",
    "        \n",
    "        return Enc_in, Enc_mask, Dec_in, Dec_mask, Recur_data.float(), Recur_mask, Drug_data, IDs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train = np.setdiff1d(ID_train,np.array(ID_list)[(10-TF_in_mask.sum(axis = [1,2]) == 1)])\n",
    "ID_test = np.setdiff1d(ID_test,np.array(ID_list)[(10-TF_in_mask.sum(axis = [1,2]) == 1)])\n",
    "print(len(ID_train),len(ID_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_recur = Table_Recur_now.index[Table_Recur_now.max(axis = 1) == 1]\n",
    "ID_normal = Table_Recur_now.index[Table_Recur_now.max(axis = 1) == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "BART_sampler_use = BART_sampler(0.2)\n",
    "recur_model_dataloader = DataLoader(BART_predict_Dataset([TF_in_data, TF_in_mask, ID_list, ID_year_list, Table_Recur_now, Table_D, ID_recur]),\n",
    "                            batch_size=batch_size, shuffle=True, collate_fn=BART_sampler_use.BART_rand_fn)\n",
    "normal_model_dataloader = DataLoader(BART_predict_Dataset([TF_in_data, TF_in_mask, ID_list, ID_year_list, Table_Recur_now, Table_D, ID_normal]),\n",
    "                            batch_size=batch_size, shuffle=True, collate_fn=BART_sampler_use.BART_rand_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio,bias = True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0\n",
    "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
    "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
    "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim,bias = bias) # Query 값에 적용될 FC 레이어\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim,bias = bias) # Key 값에 적용될 FC 레이어\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim,bias = bias) # Value 값에 적용될 FC 레이어\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim,bias = bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
    "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "        # Attention Energy 계산\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.hidden_dim)\n",
    "        energy = torch.clamp(energy,max = 1e6, min = -1e6)\n",
    "        # 마스크(mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1), -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention\n",
    "    \n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio, bias = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim,bias = bias)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim,bias = bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 하나의 임베딩이 복제되어 Query, Key, Value로 입력되는 방식\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # self attention\n",
    "        _src, attn = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # position-wise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src, attn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tok_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        device = src.device\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "\n",
    "        self.tok_embedding.to(device)\n",
    "        self.pos_embedding.to(device)\n",
    "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
    "        src = self.dropout((self.tok_embedding(src) * math.sqrt(self.hidden_dim)) + self.pos_embedding(pos))\n",
    "        \n",
    "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
    "        attns = []\n",
    "        for layer in self.layers:\n",
    "            src, attn = layer(src, src_mask)\n",
    "            attns += [attn]\n",
    "\n",
    "        return src, attns # 마지막 레이어의 출력을 반환\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # self attention\n",
    "        _trg, attn_1 = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # encoder attention\n",
    "        _trg, attn_2 = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attn_1, attn_2\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tok_embedding = nn.Linear(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        device = trg.device\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * math.sqrt(self.hidden_dim)) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        attns_1 = []\n",
    "        attns_2 = []\n",
    "        for layer in self.layers:\n",
    "            trg, attn_1, attn_2 = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            attns_1 += [attn_1]\n",
    "            attns_2 += [attn_2]\n",
    "\n",
    "        trg = self.fc_out_1(trg)\n",
    "        output = self.fc_out_2(trg)\n",
    "\n",
    "        return output, attns_1, attns_2\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_sub_mask = torch.tril(torch.ones((10, 10)),diagonal=-1).bool().T.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        Enc_in, Enc_mask, Dec_in, Dec_mask = inputs\n",
    "        \n",
    "        device = Enc_in.device\n",
    "        Dec_mask = Dec_mask.transpose(1,2) + self.trg_sub_mask.to(device)\n",
    "        Enc_mask = Enc_mask.transpose(1,2)\n",
    "\n",
    "        enc_src, enc_attns = self.encoder(Enc_in, Enc_mask)\n",
    "\n",
    "        output, dec_attns_1, dec_attns_2 = self.decoder(Dec_in, enc_src, Dec_mask, Enc_mask)\n",
    "\n",
    "        return output, enc_attns + dec_attns_1 + dec_attns_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer_pred(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio,bias = False)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio,bias = False)\n",
    "        self.drug_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio,bias = False)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio,bias = False)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask, drug_data, drug_mask, with_drug):\n",
    "        # self attention\n",
    "        _trg, attn_1 = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        if with_drug != 0:\n",
    "            _trg_2, attn_2 = self.drug_attention(trg, drug_data, drug_data, drug_mask)\n",
    "            trg = self.self_attn_layer_norm(trg + self.dropout(_trg)+ self.dropout(_trg_2))\n",
    "        else:\n",
    "            trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            attn_2 = torch.zeros_like(attn_1)\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        _trg_1, attn_3 = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg_1))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg, attn_1, attn_2, attn_3\n",
    "    \n",
    "class Decoder_pred(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, output_dim = 0):\n",
    "        super().__init__()\n",
    "        if output_dim == 0:\n",
    "            output_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tok_embedding = nn.Linear(input_dim, hidden_dim,bias = False)\n",
    "        self.drug_embedding = nn.Linear(25, hidden_dim,bias = False)\n",
    "        self.pos_embedding = nn.Embedding(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer_pred(hidden_dim, n_heads, pf_dim, dropout_ratio) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out_1 = nn.Linear(hidden_dim*10, hidden_dim,bias = False)\n",
    "        self.fc_out_2 = nn.Linear(hidden_dim, output_dim,bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask,drug_data= 0, drug_mask= 0, with_drug=0):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "        device = trg.device\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "\n",
    "        # pos: [batch_size, trg_len]\n",
    "        trg = self.dropout((self.tok_embedding(trg) * math.sqrt(self.hidden_dim)) + self.pos_embedding(pos))\n",
    "        if with_drug != 0:\n",
    "            drug_data = self.drug_embedding(drug_data)\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        attns_1 = []\n",
    "        attns_2 = []\n",
    "        attns_3 = []\n",
    "        trgs = [trg]\n",
    "        for layer in self.layers:\n",
    "            # 소스 마스크와 타겟 마스크 모두 사용\n",
    "            trg, attn_1, attn_2, attn_3 = layer(trg, enc_src, trg_mask, src_mask, drug_data, drug_mask, with_drug = with_drug)\n",
    "            attns_1 += [attn_1]\n",
    "            attns_2 += [attn_2]\n",
    "            attns_3 += [attn_3]\n",
    "            trgs += [trg]\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "        trg = trg.reshape(trg.shape[0],-1)\n",
    "        trg = nn.GELU()(self.fc_out_1(trg))\n",
    "        output = self.fc_out_2(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attns_1, attns_2, attns_3, trgs, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_pred(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_sub_mask = torch.tril(torch.ones((10, 10)),diagonal=-1).bool().T.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, inputs, with_drug = True):\n",
    "        Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data = inputs\n",
    "        device = Enc_in.device\n",
    "        # Dec_mask = Enc_mask.transpose(1,2) + self.trg_sub_mask.to(device)\n",
    "        Dec_mask = Dec_mask.transpose(1,2) + self.trg_sub_mask.to(device)\n",
    "        Enc_mask = Enc_mask.transpose(1,2)\n",
    "        drug_mask = torch.zeros_like(Dec_mask) + self.trg_sub_mask.to(device)\n",
    "        # Dec_in = torch.zeros_like(Dec_in)\n",
    "        Dec_in = Enc_in.clone()\n",
    "        enc_src, enc_attns = self.encoder(Enc_in, Enc_mask)\n",
    "\n",
    "        if with_drug:\n",
    "            output, dec_attns_1, dec_attns_2, dec_attns_3, trgs, trg = self.decoder(Dec_in, enc_src, Dec_mask, Enc_mask, drug_data, drug_mask, with_drug = with_drug)\n",
    "        else:\n",
    "            output, dec_attns_1, dec_attns_2, dec_attns_3, trgs, trg = self.decoder(Dec_in, enc_src, Dec_mask, Enc_mask, with_drug = with_drug)\n",
    "            \n",
    "        return output, enc_attns + dec_attns_1 + dec_attns_2 + dec_attns_3, trgs, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [torch.device(f\"cuda:{i}\") for i in range(2, torch.cuda.device_count()-1)]\n",
    "device = devices[0]\n",
    "with open(f\"./model_save/230913_pretrained_BART.pth\",\"rb\") as fw:\n",
    "    BART_model = torch.load(fw, map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim, layer_num, heads, ff_dim, dropout_ratio = 100, 512, 1, 8, 1024, 0\n",
    "Trained_Encoder = BART_model.module.encoder\n",
    "for para in Trained_Encoder.parameters():\n",
    "    para.requires_grad = False\n",
    "output_dim = 128\n",
    "dec_recur = Decoder_pred(input_dim, hidden_dim, layer_num, heads, ff_dim, dropout_ratio, output_dim)\n",
    "dec_normal = Decoder_pred(input_dim, hidden_dim, layer_num, heads, ff_dim, dropout_ratio, output_dim)\n",
    "OneClass_recur_model = Transformer_pred(Trained_Encoder, dec_recur).to(device)\n",
    "OneClass_normal_model = Transformer_pred(Trained_Encoder, dec_normal).to(device)\n",
    "# optimizer = torch.optim.AdamW(Pred_BART_model.parameters(), lr=1e-6)\n",
    "optimizer_recur = torch.optim.RAdam(OneClass_recur_model.parameters(), lr=1e-4)\n",
    "optimizer_normal = torch.optim.RAdam(OneClass_normal_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_recur = torch.rand(output_dim).to(device)\n",
    "point_normal = torch.rand(output_dim).to(device)\n",
    "point_recur = (point_recur/(point_recur.pow(2).sum().sqrt())).unsqueeze(0)*10\n",
    "point_normal = (point_normal/(point_normal.pow(2).sum().sqrt())).unsqueeze(0)*10\n",
    "R_len = 1\n",
    "(point_recur-point_normal).pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Pred_BART_model, f\"{model_dir}/{save_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "save_name = f\"230914_BART_Oneclass_01\"\n",
    "tqdm_epoch = tqdm.tqdm(range(n_epochs))\n",
    "loss_train_all_list = []\n",
    "loss_test_all_list = []\n",
    "# year_weight = torch.tensor([[np.log2(i) for i in range(1,11)]]).float().to(device)\n",
    "for epoch in tqdm_epoch:\n",
    "    epoch_save = ((epoch+1) % 10 == 0)\n",
    "    loss_train_all = 0\n",
    "    loss_test_all = 0\n",
    "    OneClass_recur_model.train()\n",
    "    len_train = 0\n",
    "    for Enc_in, Enc_mask, Dec_in, Dec_mask,  Recur_data, Recur_mask, drug_data, IDs in recur_model_dataloader:\n",
    "        len_train += Enc_in.shape[0]\n",
    "        Enc_in = Enc_in.to(device) # N,10,26\n",
    "        Dec_in = Dec_in.to(device) # N,10,26\n",
    "        Enc_mask = Enc_mask.to(device) # N,10,1\n",
    "        Dec_mask = Dec_mask.to(device) # N,10,1\n",
    "        Recur_data= Recur_data.to(device)\n",
    "        Recur_mask= Recur_mask.to(device)\n",
    "        drug_data = drug_data.to(device)\n",
    "        A,_,_,_ = OneClass_recur_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug  = True)\n",
    "        # loss = ((Recur_mask[:,1:] == 1)*(torch.sigmoid(A.squeeze()) - Recur_data[:,1:])).pow(2).sum()/((Recur_mask[:,1:] == 1).sum())\n",
    "        loss = torch.max((A-point_recur).pow(2).sum(axis = 1)-R_len,torch.zeros(A.shape[0]).to(device)).mean()\n",
    "        # loss = ((Recur_mask == 1)*(torch.sigmoid(A.squeeze()) - Recur_data)).pow(2).sum()/((Recur_mask == 1).sum())\n",
    "        optimizer_recur.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_recur.step()\n",
    "        loss_train_all += loss.item()*Enc_in.shape[0]\n",
    "    loss_train_all /= len_train\n",
    "    # if epoch_save: # Save Test as file\n",
    "    #     torch.save(Pred_BART_model, f\"{model_dir}/{save_name}.pth\")\n",
    "    discript = f\"model Oneclass | \"\n",
    "    discript_dict = {}\n",
    "    discript += f\"Loss :\"\n",
    "    discript += f\"Train : {loss_train_all:.3f} # {len_train}\"\n",
    "    tqdm_epoch.set_description(discript)\n",
    "    loss_train_all_list += [loss_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "save_name = f\"230914_BART_Oneclass_01\"\n",
    "tqdm_epoch = tqdm.tqdm(range(n_epochs))\n",
    "# loss_train_all_list = []\n",
    "# loss_test_all_list = []\n",
    "# year_weight = torch.tensor([[np.log2(i) for i in range(1,11)]]).float().to(device)\n",
    "for epoch in tqdm_epoch:\n",
    "    epoch_save = ((epoch+1) % 10 == 0)\n",
    "    loss_train_all = 0\n",
    "    loss_test_all = 0\n",
    "    OneClass_normal_model.train()\n",
    "    len_test = 0\n",
    "    for Enc_in, Enc_mask, Dec_in, Dec_mask, Recur_data, Recur_mask, drug_data, IDs in normal_model_dataloader:\n",
    "        len_test += Enc_in.shape[0]\n",
    "        Enc_in = Enc_in.to(device) # N,10,26\n",
    "        Dec_in = Dec_in.to(device) # N,10,26\n",
    "        Enc_mask = Enc_mask.to(device) # N,10,1\n",
    "        Dec_mask = Dec_mask.to(device) # N,10,1\n",
    "        Recur_data= Recur_data.to(device)\n",
    "        Recur_mask= Recur_mask.to(device)\n",
    "        drug_data= drug_data.to(device)\n",
    "        A,_,_,_ = OneClass_normal_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug  = True)\n",
    "        # loss = ((Recur_mask[:,1:] == 1)*(torch.sigmoid(A.squeeze()) - Recur_data[:,1:])).pow(2).sum()/((Recur_mask[:,1:] == 1).sum())\n",
    "        loss = torch.max((A-point_normal).pow(2).sum(axis = 1)-R_len,torch.zeros(A.shape[0]).to(device)).mean()\n",
    "        optimizer_normal.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_normal.step()\n",
    "        loss_test_all += loss.item()*Enc_in.shape[0]\n",
    "    loss_test_all /= len_test\n",
    "    discript = f\"model Oneclass | \"\n",
    "    discript_dict = {}\n",
    "    discript += f\"Loss :\"\n",
    "    discript += f\"Normal : {loss_test_all:.3f} # {len_test}\"\n",
    "    tqdm_epoch.set_description(discript)\n",
    "    loss_test_all_list += [loss_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A-point_normal).pow(2).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_all_list[-140:],'b')\n",
    "plt.plot(loss_test_all_list[-140:],'r')\n",
    "plt.legend(['train','validate'])\n",
    "plt.xlabel('epoch (#)')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneClass_recur_model.eval()\n",
    "OneClass_normal_model.eval()\n",
    "Result_recurs = []\n",
    "Result_normals = []\n",
    "for Enc_in, Enc_mask, Dec_in, Dec_mask,  Recur_data, Recur_mask, drug_data, IDs in recur_model_dataloader:\n",
    "    len_train += Enc_in.shape[0]\n",
    "    Enc_in = Enc_in.to(device) # N,10,26\n",
    "    Dec_in = Dec_in.to(device) # N,10,26\n",
    "    Enc_mask = Enc_mask.to(device) # N,10,1\n",
    "    Dec_mask = Dec_mask.to(device) # N,10,1\n",
    "    Recur_data= Recur_data.to(device)\n",
    "    Recur_mask= Recur_mask.to(device)\n",
    "    drug_data = drug_data.to(device)\n",
    "    A,_,_,_ = OneClass_recur_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug  = True)\n",
    "    B,_,_,_ = OneClass_normal_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug  = True)\n",
    "    # loss = ((Recur_mask[:,1:] == 1)*(torch.sigmoid(A.squeeze()) - Recur_data[:,1:])).pow(2).sum()/((Recur_mask[:,1:] == 1).sum())\n",
    "    # Result_recur = torch.max((A-point_recur).pow(2).sum(axis = 1)-R_len,torch.zeros(A.shape[0]).to(device))\n",
    "    # Result_normal = torch.max((B-point_normal).pow(2).sum(axis = 1)-R_len,torch.zeros(B.shape[0]).to(device))\n",
    "    Result_recur = (A-point_recur).pow(2).sum(axis = 1)\n",
    "    Result_normal = (B-point_normal).pow(2).sum(axis = 1)\n",
    "    Result_recurs = Result_recur.detach().cpu().numpy() if len(Result_recurs) == 0 else np.concatenate([Result_recurs,Result_recur.detach().cpu().numpy()],axis = 0)\n",
    "    Result_normals = Result_normal.detach().cpu().numpy() if len(Result_normals) == 0 else np.concatenate([Result_normals,Result_normal.detach().cpu().numpy()],axis = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_recurs2 = []\n",
    "Result_normals2 = []\n",
    "for Enc_in, Enc_mask, Dec_in, Dec_mask,  Recur_data, Recur_mask, drug_data, IDs in normal_model_dataloader:\n",
    "    len_train += Enc_in.shape[0]\n",
    "    Enc_in = Enc_in.to(device) # N,10,26\n",
    "    Dec_in = Dec_in.to(device) # N,10,26\n",
    "    Enc_mask = Enc_mask.to(device) # N,10,1\n",
    "    Dec_mask = Dec_mask.to(device) # N,10,1\n",
    "    Recur_data= Recur_data.to(device)\n",
    "    Recur_mask= Recur_mask.to(device)\n",
    "    drug_data = drug_data.to(device)\n",
    "    A,_,_,_ = OneClass_recur_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug  = True)\n",
    "    B,_,_,_ = OneClass_normal_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug  = True)\n",
    "    # loss = ((Recur_mask[:,1:] == 1)*(torch.sigmoid(A.squeeze()) - Recur_data[:,1:])).pow(2).sum()/((Recur_mask[:,1:] == 1).sum())\n",
    "    # Result_recur = torch.max((A-point_recur).pow(2).sum(axis = 1)-R_len,torch.zeros(A.shape[0]).to(device))\n",
    "    # Result_normal = torch.max((B-point_normal).pow(2).sum(axis = 1)-R_len,torch.zeros(B.shape[0]).to(device))\n",
    "    Result_recur = (A-point_recur).pow(2).sum(axis = 1)\n",
    "    Result_normal = (B-point_normal).pow(2).sum(axis = 1)\n",
    "    Result_recurs2 = Result_recur.detach().cpu().numpy() if len(Result_recurs2) == 0 else np.concatenate([Result_recurs2,Result_recur.detach().cpu().numpy()],axis = 0)\n",
    "    Result_normals2 = Result_normal.detach().cpu().numpy() if len(Result_normals2) == 0 else np.concatenate([Result_normals2,Result_normal.detach().cpu().numpy()],axis = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Result_normals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Result_recurs2-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((Result_recurs2-1)*(Result_recurs2>1))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.exp((-1)*(Result_recurs-1)*(Result_recurs>1))/(np.exp((-1)*(Result_recurs-1)*(Result_recurs>1))+np.exp((-1)*(Result_normals-1)*(Result_normals>1))),alpha = 0.2)\n",
    "plt.show()\n",
    "plt.hist(np.exp((-1)*(Result_recurs2-1)*(Result_recurs2>1))/(np.exp((-1)*(Result_recurs2-1)*(Result_recurs2>1))+np.exp((-1)*(Result_normals2-1)*(Result_normals2>1))),alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(np.exp((-1)*(Result_recurs))/(np.exp((-1)*(Result_recurs))+np.exp((-1)*(Result_normals))), color = 'r', bins = [i/20 for i in range(21)])\n",
    "plt.title(\"Recurred patients\")\n",
    "plt.xticks([])\n",
    "plt.xlim(0,1)\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(np.exp((-1)*(Result_recurs2))/(np.exp((-1)*(Result_recurs2))+np.exp((-1)*(Result_normals2))), color = 'b', bins = [i/20 for i in range(21)])\n",
    "plt.title(\"Non-recurred patients\")\n",
    "plt.xlim(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.exp((-1)*(Result_recurs)),np.exp((-1)*(Result_normals)), s = 1,c='r', alpha = 0.5)\n",
    "plt.scatter(np.exp((-1)*(Result_recurs2)),np.exp((-1)*(Result_normals2)), s = 1,c='b', alpha = 0.5)\n",
    "# plt.xlim(0,2.1)\n",
    "# plt.ylim(0,2.1)\n",
    "plt.xlabel('recur predict')\n",
    "plt.ylabel('normal predict')\n",
    "plt.legend([\"Recur\",\"Normal\"])\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.exp((-1)*(Result_recurs-1)*(Result_recurs>1)),np.exp((-1)*(Result_normals-1)*(Result_normals>1)), s = 1,c='r', alpha = 0.5)\n",
    "plt.scatter(np.exp((-1)*(Result_recurs2-1)*(Result_recurs2>1)),np.exp((-1)*(Result_normals2-1)*(Result_normals2>1)), s = 1,c='b', alpha = 0.5)\n",
    "# plt.xlim(0,2.1)\n",
    "# plt.ylim(0,2.1)\n",
    "plt.xlabel('recur predict')\n",
    "plt.ylabel('normal predict')\n",
    "plt.legend([\"Recur\",\"Normal\"])\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(1/Result_recurs),np.log(1/Result_normals), s = 1,c='r')\n",
    "plt.scatter(np.log(1/Result_recurs2),np.log(1/Result_normals2), s = 1,c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss :Train : 0.235 # 4103 Validate : 0.329\n",
    "# Loss :Train : 0.211 # 4103 Validate : 0.261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_BART_model.eval()\n",
    "A_all_val = []\n",
    "a_all_val = []\n",
    "b_all_val = []\n",
    "c_all_val = []\n",
    "d_all_val = []\n",
    "B_all_val = []\n",
    "D_all_val = []\n",
    "# C_all_val = []\n",
    "out_all_val = []\n",
    "out_mas_val = []\n",
    "ID_all = []\n",
    "BART_sampler_use.rand_p = 0\n",
    "for Enc_in, Enc_mask, Dec_in, Dec_mask, Recur_data, Recur_mask, drug_data, IDs in train_dataloader:\n",
    "    len_test += Enc_in.shape[0]\n",
    "    Enc_in = Enc_in.to(device) # N,10,26\n",
    "    Dec_in = Dec_in.to(device) # N,10,26\n",
    "    Enc_mask = Enc_mask.to(device) # N,10,1\n",
    "    Dec_mask = Dec_mask.to(device) # N,10,1\n",
    "    Recur_data= Recur_data.to(device)\n",
    "    Recur_mask= Recur_mask.to(device)\n",
    "    drug_data= drug_data.to(device)\n",
    "    A,B,C,D = Pred_BART_model([Enc_in, Enc_mask, Enc_in, Enc_mask, drug_data], with_drug = True)\n",
    "    # A,B = Pred_BART_model([Enc_in, Enc_mask, Recur_data[:,:-1].unsqueeze(2), ~Recur_mask[:,:-1].unsqueeze(2)])\n",
    "    A = torch.sigmoid(torch.clamp(A.squeeze(),max = 1e6, min = -1e6))\n",
    "    ID_all += IDs\n",
    "    # B_all_val = B.detach().cpu().numpy() if len(B_all_val) == 0 else np.concatenate([B_all_val,B.detach().cpu().numpy()],axis = 0)\n",
    "    D_all_val = D.detach().cpu().numpy() if len(D_all_val) == 0 else np.concatenate([D_all_val,D.detach().cpu().numpy()],axis = 0)\n",
    "    # B_all_val = [i.detach().cpu().numpy() for i in B] if len(B_all_val) == 0 else [np.concatenate([i,j.detach().cpu().numpy()],axis = 0) for i,j in zip(B_all_val,B)]\n",
    "    A_all_val = A.detach().cpu().numpy() if len(A_all_val) == 0 else np.concatenate([A_all_val,A.detach().cpu().numpy()],axis = 0)\n",
    "    # C_all_val = [i.detach().cpu().numpy() for i in C] if len(C_all_val) == 0 else [np.concatenate([i,j.detach().cpu().numpy()],axis = 0) for i,j in zip(C_all_val,C)]\n",
    "    a_all_val = Enc_in.detach().cpu().numpy() if len(a_all_val) == 0 else np.concatenate([a_all_val,Enc_in.detach().cpu().numpy()],axis = 0)\n",
    "    b_all_val = Dec_in.detach().cpu().numpy() if len(b_all_val) == 0 else np.concatenate([b_all_val,Dec_in.detach().cpu().numpy()],axis = 0)\n",
    "    c_all_val = Enc_mask.detach().cpu().numpy() if len(c_all_val) == 0 else np.concatenate([c_all_val,Enc_mask.detach().cpu().numpy()],axis = 0)\n",
    "    d_all_val = Dec_mask.detach().cpu().numpy() if len(d_all_val) == 0 else np.concatenate([d_all_val,Dec_mask.detach().cpu().numpy()],axis = 0)\n",
    "    out_all_val = Recur_data.detach().cpu().numpy() if len(out_all_val) == 0 else np.concatenate([out_all_val,Recur_data.detach().cpu().numpy()],axis = 0)\n",
    "    out_mas_val = Recur_mask.detach().cpu().numpy() if len(out_mas_val) == 0 else np.concatenate([out_mas_val,Recur_mask.detach().cpu().numpy()],axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_BART_model.eval()\n",
    "D_all_val_test = []\n",
    "# C_all_val = []\n",
    "A_all_val_test = []\n",
    "out_all_val_test = []\n",
    "out_mas_val_test = []\n",
    "c_all_val_test = []\n",
    "for Enc_in, Enc_mask, Dec_in, Dec_mask, Recur_data, Recur_mask, drug_data, IDs in test_dataloader:\n",
    "    len_test += Enc_in.shape[0]\n",
    "    Enc_in = Enc_in.to(device) # N,10,26\n",
    "    Dec_in = Dec_in.to(device) # N,10,26\n",
    "    Enc_mask = Enc_mask.to(device) # N,10,1\n",
    "    Dec_mask = Dec_mask.to(device) # N,10,1\n",
    "    Recur_data= Recur_data.to(device)\n",
    "    Recur_mask= Recur_mask.to(device)\n",
    "    drug_data= drug_data.to(device)\n",
    "    A,B,C,D = Pred_BART_model([Enc_in, Enc_mask, Enc_in, Enc_mask, drug_data], with_drug = True)\n",
    "    A = torch.sigmoid(torch.clamp(A.squeeze(),max = 1e6, min = -1e6))\n",
    "    A_all_val_test = A.detach().cpu().numpy() if len(A_all_val_test) == 0 else np.concatenate([A_all_val_test,A.detach().cpu().numpy()],axis = 0)\n",
    "    D_all_val_test = D.detach().cpu().numpy() if len(D_all_val_test) == 0 else np.concatenate([D_all_val_test,D.detach().cpu().numpy()],axis = 0)\n",
    "    c_all_val_test = Enc_mask.detach().cpu().numpy() if len(c_all_val_test) == 0 else np.concatenate([c_all_val_test,Enc_mask.detach().cpu().numpy()],axis = 0)\n",
    "    out_all_val_test = Recur_data.detach().cpu().numpy() if len(out_all_val_test) == 0 else np.concatenate([out_all_val_test,Recur_data.detach().cpu().numpy()],axis = 0)\n",
    "    out_mas_val_test = Recur_mask.detach().cpu().numpy() if len(out_mas_val_test) == 0 else np.concatenate([out_mas_val_test,Recur_mask.detach().cpu().numpy()],axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(out_all_val[:,1:].reshape(-1)[(out_mas_val[:,1:].reshape(-1)==1)], A_all_val.reshape(-1)[(out_mas_val[:,1:].reshape(-1)==1)])\n",
    "AUC_train = roc_auc_score(out_all_val[:,1:].reshape(-1)[(out_mas_val[:,1:].reshape(-1)==1)], A_all_val.reshape(-1)[(out_mas_val[:,1:].reshape(-1)==1)])\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(out_all_val_test[:,1:].reshape(-1)[(out_mas_val_test[:,1:].reshape(-1)==1)], A_all_val_test.reshape(-1)[(out_mas_val_test[:,1:].reshape(-1)==1)])\n",
    "AUC_test = roc_auc_score(out_all_val_test[:,1:].reshape(-1)[(out_mas_val_test[:,1:].reshape(-1)==1)], A_all_val_test.reshape(-1)[(out_mas_val_test[:,1:].reshape(-1)==1)])\n",
    "plt.plot(fpr, tpr,'b')\n",
    "plt.plot(fpr_test, tpr_test,'r')\n",
    "plt.xlabel('FP Rate')\n",
    "plt.ylabel('TP Rate')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend([f\"Train : {AUC_train:.3f}\",f\"Test : {AUC_test:.3f}\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "plt.figure(figsize= (20,8))\n",
    "for i in range(2,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    fpr, tpr, thresholds = roc_curve(out_all_val[:,i].reshape(-1)[(out_mas_val[:,i].reshape(-1)==1)], A_all_val[:,i-1].reshape(-1)[(out_mas_val[:,i].reshape(-1)==1)])\n",
    "    AUC_train = roc_auc_score(out_all_val[:,i].reshape(-1)[(out_mas_val[:,i].reshape(-1)==1)], A_all_val[:,i-1].reshape(-1)[(out_mas_val[:,i].reshape(-1)==1)])\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(out_all_val_test[:,i].reshape(-1)[(out_mas_val_test[:,i].reshape(-1)==1)], A_all_val_test[:,i-1].reshape(-1)[(out_mas_val_test[:,i].reshape(-1)==1)])\n",
    "    AUC_test = roc_auc_score(out_all_val_test[:,i].reshape(-1)[(out_mas_val_test[:,i].reshape(-1)==1)], A_all_val_test[:,i-1].reshape(-1)[(out_mas_val_test[:,i].reshape(-1)==1)])\n",
    "    plt.plot(fpr, tpr,'b')\n",
    "    plt.plot(fpr_test, tpr_test,'r')\n",
    "    plt.xlabel('FP Rate')\n",
    "    plt.ylabel('TP Rate')\n",
    "    plt.title(f\"ROC curve | {i} year\")\n",
    "    plt.legend([f\"Train : {AUC_train:.3f}\",f\"Test : {AUC_test:.3f}\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Last_weight = Pred_BART_model.decoder.fc_out_2.weight.unsqueeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_all_val = D_all_val * Last_weight\n",
    "D_all_val_test = D_all_val_test * Last_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_mean = np.mean(D_all_val, axis = 0)[np.newaxis,:,:]\n",
    "D_std = np.std(D_all_val, axis = 0)[np.newaxis,:,:]\n",
    "D_std[D_std == 0] = 1\n",
    "D_all_val = (D_all_val-D_mean)/D_std\n",
    "D_all_val_test = (D_all_val_test-D_mean)/D_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PCA_model = PCA(n_components=100)\n",
    "PCA_result_train = PCA_model.fit_transform(D_all_val.reshape(-1,5120))\n",
    "ssq = np.cumsum(PCA_model.explained_variance_ratio_)\n",
    "lv = np.argmin(np.abs(ssq - 0.75)) + 1\n",
    "lv=30\n",
    "ssq_ = ssq[lv - 1]\n",
    "PCA_result_train = PCA_result_train[:, :lv]\n",
    "loading_pca = PCA_model.components_[:lv, :].T\n",
    "print(f\"LV: {lv} (% variance explained: {ssq_*100:.2f}%)\")\n",
    "PCA_result_test = PCA_model.transform(D_all_val_test.reshape(-1,5120))\n",
    "PCA_result_test = PCA_result_test[:, :lv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssq = np.cumsum(PCA_model.explained_variance_ratio_)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(ssq,'*-')\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-1,51)\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssq = (PCA_model.explained_variance_ratio_)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(ssq,'*-')\n",
    "# plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-1,51)\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_model = UMAP()\n",
    "UMAP_result = UMAP_model.fit_transform(PCA_result_train)\n",
    "UMAP_result_test = UMAP_model.transform(PCA_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1)\n",
    "plt.title('Trainset')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(UMAP_result_test[:,0],UMAP_result_test[:,1], s = 1)\n",
    "plt.title('Validateset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1, c = ['r' if i > 0 else 'b' for i in np.sum(out_all_val, axis = -1)])\n",
    "plt.xlim(min(np.min(UMAP_result[:,0]),np.min(UMAP_result_test[:,0]))-0.5,max(np.max(UMAP_result[:,0]),np.max(UMAP_result_test[:,0]))+0.5)\n",
    "plt.ylim(min(np.min(UMAP_result[:,1]),np.min(UMAP_result_test[:,1]))-0.5,max(np.max(UMAP_result[:,1]),np.max(UMAP_result_test[:,1]))+0.5)\n",
    "plt.grid(1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(UMAP_result_test[:,0],UMAP_result_test[:,1], s = 1, c = ['r' if i > 0 else 'b' for i in np.sum(out_all_val_test, axis = -1)])\n",
    "plt.xlim(min(np.min(UMAP_result[:,0]),np.min(UMAP_result_test[:,0]))-0.5,max(np.max(UMAP_result[:,0]),np.max(UMAP_result_test[:,0]))+0.5)\n",
    "plt.ylim(min(np.min(UMAP_result[:,1]),np.min(UMAP_result_test[:,1]))-0.5,max(np.max(UMAP_result[:,1]),np.max(UMAP_result_test[:,1]))+0.5)\n",
    "plt.grid(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    " \n",
    "X = D_all_val.reshape(-1,5120)\n",
    " \n",
    "kmeans = KMeans(n_clusters=12, random_state=0) ## KMeans 초기화\n",
    "kmeans.fit(X) ## 클러스터링 알고리즘 수행\n",
    " \n",
    "labels = kmeans.labels_ ## 클러스터링 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for ID in ID_all:\n",
    "    if ID in Table_2[\"new_ID\"]:\n",
    "        l += [Table_2[(Table_2[\"new_ID\"] == ID)*(Table_2[\"count_idx\"] == 0)].Test_date_normed.values[0]]\n",
    "    else:\n",
    "        l += [np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2[\"Cluster_num\"] = ['' for _ in range(len(Table_2))]\n",
    "for ID,clust in zip(ID_all,labels):\n",
    "    if ID in Table_2.index:\n",
    "        Table_2.loc[ID,[\"Cluster_num\"]] = [clust for _ in range(len(Table_2.loc[[ID]]))]\n",
    "Table_2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2_sub[Table_2_sub.columns[:-3]].replace(0\t, np.nan, inplace=True)\n",
    "Table_2_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2_sub = Table_2.groupby(level = 0).count()\n",
    "Table_2_sub.replace(0\t, np.nan, inplace=True)\n",
    "Table_2_sub[\"Data_count\"] = [0 for _ in range(len(Table_2_sub))]\n",
    "for n,ID in enumerate(ID_all):\n",
    "    Table_2_sub.loc[ID,\"Data_count\"] = color_now[n]\n",
    "Table_2_sub.groupby(\"Data_count\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.imshow(((Table_2_sub.groupby(\"Data_count\").count().values)/(Table_2_sub.groupby(\"Data_count\").count()[\"new_ID\"].values).reshape(11,1))[1:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2_sub.groupby(\"Data_count\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1 = Table_2.iloc[[n for n,i in enumerate(Table_2[\"Cluster_num\"].values) if i in [0,7] ]]\n",
    "Table_case_1 = Table_case_1[Table_case_1.columns[1:]]\n",
    "Table_case_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Table_case_1[Table_case_1[\"count_idx\"] == 0][\"Cluster_num\"] == 5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1 = Table_2.iloc[[n for n,i in enumerate(Table_2[\"Cluster_num\"].values) if i in range(12) ]]\n",
    "\n",
    "Table_case_1[Table_case_1.columns[-2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2[Table_2[\"count_idx\"]==0][\"Cluster_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2[\"count_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=Table_2[Table_2[\"count_idx\"]==0], x=\"Cluster_num\", y=\"Test_date_normed\", order = [i for i in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([Table_case_1[Table_case_1[\"Cluster_num\"] ==0]['Test_date_normed'].values,Table_case_1[Table_case_1[\"Cluster_num\"] ==7]['Test_date_normed'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,3))\n",
    "plt.imshow(Table_case_1[Table_case_1[\"count_idx\"] == 0].sort_values(\"Cluster_num\").groupby(\"Cluster_num\").count().values, vmax = 100)\n",
    "plt.xticks([i*5 for i in range(27)])\n",
    "plt.show()\n",
    "plt.figure(figsize = (15,3))\n",
    "plt.imshow(Table_case_1[Table_case_1[\"count_idx\"] == 0].sort_values(\"Cluster_num\").groupby(\"Cluster_num\").mean().values)\n",
    "plt.xticks([i*5 for i in range(27)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1.columns[[24,28,30,33,39,42,45,63,75]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1[Table_case_1[\"count_idx\"] == 0].sort_values(\"Cluster_num\").groupby(\"Cluster_num\").mean()[['RDW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1[Table_case_1[\"count_idx\"] == 0].sort_values(\"Cluster_num\").groupby(\"Cluster_num\").count()[['CRP (C-Reactive Protein)', 'Delta neutrophil', 'ESR', 'LUC(#)', 'MPC',\n",
    "       'PDW(%)', 'PMN%', 'Free Fatty Acid', 'tCO2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1.columns[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,15))\n",
    "plt.imshow(((Table_case_1[Table_case_1[\"count_idx\"] == 0].sort_values(\"Cluster_num\")+10).fillna(0)).values[:,1:])\n",
    "# plt.yticks([0,575,1047,1463],[0,575,1047,1463])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_case_1[Table_case_1[\"count_idx\"] == 0].sort_values(\"Cluster_num\").fillna(0).values[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1, c = l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "for i in range(12):\n",
    "    plt.subplot(2,6,i+1)\n",
    "    plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1, c = labels, alpha = [1 if j == i else 0.1 for j in labels])\n",
    "    plt.title(f\"cluster {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Table_2[[\"count_idx\",cate]].groupby(\"count_idx\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in Table_2.columns[2:]:\n",
    "    idx = 0\n",
    "    if len(Table_2[[\"count_idx\",cate]].groupby(\"count_idx\").first().dropna()) == 1:\n",
    "        plt.figure(figsize = (6,2))\n",
    "        c_mean = []\n",
    "        c_min = []\n",
    "        c_max = []\n",
    "        for cluster_idx in range(12):\n",
    "            cluster_IDs = np.array(ID_all)[np.where(labels == cluster_idx)[0]]\n",
    "            cluster_IDs = np.array(ID_all)[np.where(labels == cluster_idx)[0]]\n",
    "            cluster_table = Table_2.iloc[[n for n,i in enumerate(Table_2[\"new_ID\"].values) if i in cluster_IDs]][[\"count_idx\",cate]]\n",
    "            c_mean += [cluster_table.groupby(\"count_idx\").mean().dropna().values[0,0]]\n",
    "            c_min += [cluster_table.groupby(\"count_idx\").min().dropna().values[0,0]]\n",
    "            c_max += [cluster_table.groupby(\"count_idx\").max().dropna().values[0,0]]\n",
    "        plt.bar([i for i in range(12)], c_mean)\n",
    "        plt.scatter([i for i in range(12)], c_min)\n",
    "        plt.scatter([i for i in range(12)], c_max)\n",
    "        plt.xlabel(\"clusters\")\n",
    "        plt.title(cate)\n",
    "        plt.show()\n",
    "        \n",
    "    else:      \n",
    "        plt.figure(figsize = (12,4))\n",
    "        for cluster_idx in range(12):\n",
    "            cluster_IDs = np.array(ID_all)[np.where(labels == cluster_idx)[0]]\n",
    "            val_min = Table_2[cate].min()-0.5\n",
    "            val_max = Table_2[cate].max()+0.5\n",
    "            cluster_table = Table_2.iloc[[n for n,i in enumerate(Table_2[\"new_ID\"].values) if i in cluster_IDs]][[\"count_idx\",cate]]\n",
    "            c_mean = cluster_table.groupby(\"count_idx\").mean()\n",
    "            c_min = cluster_table.groupby(\"count_idx\").min()\n",
    "            c_max = cluster_table.groupby(\"count_idx\").max()\n",
    "            for i in range(-1,9):\n",
    "                if i not in c_mean.index:\n",
    "                    c_mean.loc[i] = [np.nan for _ in range(c_mean.shape[1])]\n",
    "                    c_min.loc[i] = [np.nan for _ in range(c_min.shape[1])]\n",
    "                    c_max.loc[i] = [np.nan for _ in range(c_max.shape[1])]\n",
    "            idx += 1\n",
    "            plt.subplot(2,6,idx)\n",
    "            plt.plot([i for i in range(-1,9)],c_mean.loc[[i for i in range(-1,9)]][cate])\n",
    "            plt.xlim(-1.1,8.1)\n",
    "            plt.ylim(val_min,val_max)\n",
    "            plt.fill_between([i for i in range(-1,9)],c_min.loc[[i for i in range(-1,9)]][cate],c_max.loc[[i for i in range(-1,9)]][cate],alpha = 0.3)\n",
    "        plt.suptitle(cate)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1, c = labels)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_now = np.array([10-np.sum(c_all_val,axis = (1,2)) for _ in range(10)]).T.reshape(-1)[~c_all_val.reshape(-1)]\n",
    "plt.figure(figsize = (15,10))\n",
    "for idx in range(1,11):\n",
    "    plt.subplot(3,4,idx)\n",
    "    plt.scatter(UMAP_result[~c_all_val.reshape(-1) ,0],UMAP_result[~c_all_val.reshape(-1),1], alpha = [0.5 if i == idx else 0 for i in color_now], s = .5, c = color_now)\n",
    "    plt.title(f\"Data count : {idx}\")\n",
    "# plt.colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_now = np.array([[i for i in range(10)] for _ in range(len(out_all_val))]).reshape(-1)[~c_all_val.reshape(-1)]\n",
    "plt.figure(figsize = (15,10))\n",
    "for idx in range(1,11):\n",
    "    plt.subplot(3,4,idx)\n",
    "    plt.scatter(UMAP_result[~c_all_val.reshape(-1) ,0],UMAP_result[~c_all_val.reshape(-1),1], alpha = [0.5 if i == (idx-1) else 0 for i in color_now], s = .5, c = color_now)\n",
    "    plt.title(f\"Timepoint (year after Op) : {idx}\")\n",
    "# plt.colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_result_D = UMAP_model.transform(D.detach().cpu().numpy().reshape(-1,512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = train_dataloader.dataset.__getitem__(1387)\n",
    "Sample[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enc_in = Sample[\"data\"].unsqueeze(0).repeat(10,1,1).to(device) # 1,10,50\n",
    "Enc_mask = torch.tril(torch.ones((10, 10)), diagonal = -1).T.unsqueeze(2).to(device).bool() # 10,10,1\n",
    "Enc_in = Enc_in * (Enc_mask == 0)\n",
    "Dec_in = torch.zeros_like(Enc_in).to(device) # 1,10,50\n",
    "Dec_mask = torch.zeros_like(Enc_mask).to(device).bool() # 1,10,50\n",
    "drug_data = Sample[\"Drug_data\"].unsqueeze(0).repeat(10,1,1).to(device)\n",
    "A,B,C,D = Pred_BART_model([Enc_in, Enc_mask, Dec_in, Dec_mask, drug_data], with_drug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enc_mask[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(train_dataloader.dataset.ID_list==\"P_003083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mas_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2 = pd.read_excel('230913_concat_table.xlsx')\n",
    "Table_2.index = Table_2[\"new_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_result[color_now == 0,0],UMAP_result[color_now == 0,1], alpha = 0.3, s = 0.5, c = label_use[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2 = Table_2[Table_2[\"count_idx_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_1[\"count_in_TF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_use[i].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_use = Table_2.loc[ID_all].groupby(level = 0).max()\n",
    "plt.figure(figsize = (17,15))\n",
    "idx = 0\n",
    "for i in label_use:\n",
    "    idx+=1\n",
    "    plt.subplot(5,5,idx)\n",
    "    try:\n",
    "        if len(label_use[i].unique())<3:\n",
    "            plt.scatter(UMAP_result[:,0],UMAP_result[:,1], alpha = [1 if j >0.5 else 0 for j in label_use[i]], s = 0.5, c = label_use[i])        \n",
    "        else:\n",
    "            plt.scatter(UMAP_result[:,0],UMAP_result[:,1], alpha = 0.3, s = 0.5, c = label_use[i])\n",
    "        plt.colorbar()\n",
    "        # plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1, c = label_use[i], alpha = 0.3)\n",
    "    except:\n",
    "        pass\n",
    "    plt.title(i)\n",
    "    if idx == 25:\n",
    "        idx = 0\n",
    "        plt.show()\n",
    "        plt.figure(figsize = (15,15))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_result[: ,0],UMAP_result[:,1], c = l, s = 1, alpha =0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_now = (10-np.sum(c_all_val,axis = (1,2))).T.reshape(-1)[:]\n",
    "plt.figure(figsize = (15,10))\n",
    "idx = 0\n",
    "for i in range(10):\n",
    "    idx+=1\n",
    "    plt.subplot(3,4,idx)\n",
    "    plt.scatter(UMAP_result[: ,0],UMAP_result[:,1], c = l, alpha = [1 if (j>i*0.5-3)*(j<=i*0.5-3+0.5) else 0 for j in l], s = 1)\n",
    "# plt.colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_out = np.array(ID_all)[color_now == 1]\n",
    "ID_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([Table_2.loc[np.array(ID_all)[color_now == i]][\"Test_date_normed\"] for i in range(1,11)])\n",
    "plt.xlabel('Data Count')\n",
    "plt.ylabel('Relative Operation Date')\n",
    "plt.xticks([i+1 for i in range(10)],[i+1 for i in range(10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_now = (10-np.sum(c_all_val,axis = (1,2))).T.reshape(-1)[:]\n",
    "plt.figure(figsize = (15,10))\n",
    "for idx in range(1,11):\n",
    "    plt.subplot(3,4,idx)\n",
    "    plt.scatter(UMAP_result[: ,0],UMAP_result[:,1], alpha = [0.5 if i == idx else 0 for i in color_now], s = 1)\n",
    "    plt.title(f\"Data count : {idx}\")\n",
    "# plt.colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_now = ([i for i in range(10)]).reshape(-1)[~c_all_val.reshape(-1)]\n",
    "plt.figure(figsize = (15,10))\n",
    "for idx in range(1,11):\n",
    "    plt.subplot(3,4,idx)\n",
    "    plt.scatter(UMAP_result[: ,0],UMAP_result[:,1], alpha = [0.5 if i == (idx-1) else 0 for i in color_now], s = .5, c = color_now)\n",
    "    plt.title(f\"Timepoint (year after Op) : {idx}\")\n",
    "# plt.colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_use = Table_1.loc[ID_all].groupby(level = 0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_result_test[:,0],UMAP_result_test[:,1], s = 1, c = ['r' if i > 0 else 'b' for i in np.sum(out_all_val_test, axis = -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_result[:,0],UMAP_result[:,1], s = 1, c = ['r' if i > 0 else 'b' for i in np.sum(out_all_val, axis = -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCA_result_test[:,0],PCA_result_test[:,1], s = 1, c = ['r' if i > 0 else 'b' for i in np.sum(out_all_val_test, axis = -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCA_result[:,0],PCA_result[:,1], s = 1, c = ['r' if i > 0 else 'b' for i in np.sum(out_all_val, axis = -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_model = umap.UMAP()\n",
    "UMAP_result = UMAP_model.fit_transform(data['m'][label_NAN][:,using_z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,3))\n",
    "idx = 0\n",
    "idx+=1\n",
    "plt.subplot(2,1,idx)\n",
    "plt.hist(A_all_val.reshape(-1)[(out_all_val[:,1:].reshape(-1)==0)*(out_mas_val[:,1:].reshape(-1)==1)],color='r', alpha = 0.3,bins = [i/20 for i in range(21)])\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.legend(['Prediction for No Recur'])\n",
    "idx+=1\n",
    "plt.subplot(2,1,idx)\n",
    "plt.hist(A_all_val.reshape(-1)[(out_all_val[:,1:].reshape(-1)==1)*(out_mas_val[:,1:].reshape(-1)==1)],color='b', alpha = 0.3,bins = [i/20 for i in range(21)])\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.legend(['Prediction for Recur'])\n",
    "    # plt.scatter((A_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             (out_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             s = 1, alpha = 0.4)\n",
    "    # plt.plot([-4,4],[-4,4],color = 'k')\n",
    "# plt.xticks([-4,-2,0,2,4],[-4,-2,0,2,4])\n",
    "\n",
    "#1 plt.ylabel('Answer')    \n",
    "plt.xlabel('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,3))\n",
    "idx = 0\n",
    "idx+=1\n",
    "plt.subplot(2,1,idx)\n",
    "plt.hist(A_all_val_test.reshape(-1)[(out_all_val_test[:,1:].reshape(-1)==0)*(out_mas_val_test[:,1:].reshape(-1)==1)],color='r', alpha = 0.3,bins = [i/20 for i in range(21)])\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.legend(['Prediction for No Recur'])\n",
    "idx+=1\n",
    "plt.subplot(2,1,idx)\n",
    "plt.hist(A_all_val_test.reshape(-1)[(out_all_val_test[:,1:].reshape(-1)==1)*(out_mas_val_test[:,1:].reshape(-1)==1)],color='b', alpha = 0.3,bins = [i/20 for i in range(21)])\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.legend(['Prediction for Recur'])\n",
    "    # plt.scatter((A_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             (out_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             s = 1, alpha = 0.4)\n",
    "    # plt.plot([-4,4],[-4,4],color = 'k')\n",
    "# plt.xticks([-4,-2,0,2,4],[-4,-2,0,2,4])\n",
    "\n",
    "#1 plt.ylabel('Answer')    \n",
    "plt.xlabel('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_model = umap.UMAP()\n",
    "UMAP_result = UMAP_model.fit_transform(data['m'][label_NAN][:,using_z])\n",
    "UMAP_result_test = UMAP_model.transform(test_data['m'][label_NAN_test][:,using_z])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(torch.clamp(Pred_BART_model.decoder.fc_out_2(nn.ReLU()(Pred_BART_model.decoder.fc_out_1(C[0][idx]))).squeeze(),max = 1e6, min = -1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(C)):\n",
    "    # C = [i.detach().cpu().numpy() for i in C]\n",
    "    i = 0\n",
    "    mid_x = torch.sigmoid(torch.clamp(Pred_BART_model.decoder.fc_out_2(nn.ReLU()(Pred_BART_model.decoder.fc_out_1(C[i][idx]))).squeeze(),max = 1e6, min = -1e6))\n",
    "    mid_x = mid_x.detach().cpu().numpy()\n",
    "    plt.plot(mid_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(C[0])):\n",
    "    # C = [i.detach().cpu().numpy() for i in C]\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(2,1,1)\n",
    "    for i in range(2):\n",
    "        mid_x = torch.sigmoid(torch.clamp(Pred_BART_model.decoder.fc_out_2(nn.ReLU()(Pred_BART_model.decoder.fc_out_1(C[i][idx]))).squeeze(),max = 1e6, min = -1e6))\n",
    "        mid_x = mid_x.detach().cpu().numpy()\n",
    "        plt.plot(mid_x)\n",
    "    plt.plot(Recur_data[idx,1:].detach().cpu().numpy())\n",
    "    plt.plot(Recur_mask[idx,1:].detach().cpu().numpy())\n",
    "    plt.legend(['In','Out','Ans','Mask'])\n",
    "    plt.subplot(2,1,2)\n",
    "    leg_sub = []\n",
    "    for i in range(drug_data.shape[2]):\n",
    "        if drug_data[idx,:,i].sum() > 0:\n",
    "            plt.plot(drug_data[idx,:,i].detach().cpu().numpy())\n",
    "            leg_sub += [Table_D.columns[i]]\n",
    "    plt.legend(leg_sub)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_all_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out_all_val==1).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out_all_val==0).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (7,14))\n",
    "idx = 0\n",
    "for i in range(10):\n",
    "    \n",
    "    plt.subplot(10,2,idx//2*4+1+idx%2)\n",
    "    plt.hist(A_all_val[:,i][(out_all_val[:,i+1]==0)*(out_mas_val[:,i+1]==1)],color='r', alpha = 0.3)\n",
    "    plt.xlim(-0.1,1.1)\n",
    "    # idx+=1\n",
    "    # plt.subplot(5,4,idx)\n",
    "    plt.subplot(10,2,idx//2*4+3+idx%2)\n",
    "    plt.hist(A_all_val[:,i][(out_all_val[:,i+1]==1)*(out_mas_val[:,i+1]==1)],color='b', alpha = 0.3)\n",
    "    plt.xlim(-0.1,1.1)\n",
    "    plt.xticks([0,1],[\"No Recur\", \"Recur\"])\n",
    "    plt.title(f\"{i+1} year\")\n",
    "    idx+=1\n",
    "    # plt.scatter((A_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             (out_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             s = 1, alpha = 0.4)\n",
    "    # plt.plot([-4,4],[-4,4],color = 'k')\n",
    "# plt.xticks([-4,-2,0,2,4],[-4,-2,0,2,4])\n",
    "# plt.ylabel('Answer')    \n",
    "plt.xlabel('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (7,14))\n",
    "idx = 0\n",
    "for i in range(10):\n",
    "    \n",
    "    plt.subplot(10,2,idx//2*4+1+idx%2)\n",
    "    plt.hist(A_all_val_test[:,i][(out_all_val_test[:,i+1]==0)*(out_mas_val_test[:,i+1]==1)],color='r', alpha = 0.3)\n",
    "    plt.xlim(-0.1,1.1)\n",
    "    # idx+=1\n",
    "    # plt.subplot(5,4,idx)\n",
    "    plt.subplot(10,2,idx//2*4+3+idx%2)\n",
    "    plt.hist(A_all_val_test[:,i][(out_all_val_test[:,i+1]==1)*(out_mas_val_test[:,i+1]==1)],color='b', alpha = 0.3)\n",
    "    plt.xlim(-0.1,1.1)\n",
    "    plt.xticks([0,1],[\"No Recur\", \"Recur\"])\n",
    "    plt.title(f\"{i+1} year\")\n",
    "    idx+=1\n",
    "    # plt.scatter((A_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             (out_all_val[:,i].reshape(-1)[out_mas_val[:,i].reshape(-1)==1]),\n",
    "    #             s = 1, alpha = 0.4)\n",
    "    # plt.plot([-4,4],[-4,4],color = 'k')\n",
    "# plt.xticks([-4,-2,0,2,4],[-4,-2,0,2,4])\n",
    "# plt.ylabel('Answer')    \n",
    "plt.xlabel('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(Enc_mask.sum(axis = [1,2]).detach().cpu().numpy())\n",
    "for j in range(len(B)):\n",
    "    print(j)\n",
    "    for i in range(8):\n",
    "        plt.subplot(2,4,i+1)\n",
    "        plt.imshow(B[j][idx,i].detach().cpu().numpy(), vmin = 0, vmax = 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env_yoo_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
