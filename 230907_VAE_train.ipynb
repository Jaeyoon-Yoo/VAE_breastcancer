{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "pd.set_option('mode.chained_assignment',  None)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dir = './loss'\n",
    "model_dir = './model_save'\n",
    "table_loc_master = \"/data/Storage_DAS02/jaeyoon/230101_BreastCancer_Project/data/230629_BC_new/\"\n",
    "table_data_master = \"230913_BT_Timeseries_new.xlsx\"\n",
    "table_data_2_master = \"230913_EMR_Timeseries_new.xlsx\"\n",
    "Table_BT = pd.read_excel(table_loc_master+table_data_master)\n",
    "Table_EMR = pd.read_excel(table_loc_master+table_data_2_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_BT[var].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_EMR = pd.read_excel(table_loc_master+table_data_2_master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_bi = []\n",
    "Var_con = []\n",
    "for var in Table_EMR.columns[3:]:\n",
    "    try:\n",
    "        if len(np.unique(Table_EMR[var].values)) < 10:\n",
    "            print(var)\n",
    "            for val in np.unique(Table_EMR[var].values)[:-1]:\n",
    "                print(val)\n",
    "                Table_EMR[f\"{var}_{val}\"] = (Table_EMR[var] == val)\n",
    "                Var_bi += [f\"{var}_{val}\"]\n",
    "        else:\n",
    "            Var_con += [var]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Table_BT.columns[3:]:\n",
    "    Var_con += [var]\n",
    "    if len(np.unique(Table_BT[var].values)) < 10:\n",
    "        print(var, (np.unique(Table_BT[var].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_BT.to_excel('BT_subbbb.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_ID = ''\n",
    "for i in range(len(Table_BT)):\n",
    "    if type(Table_BT.loc[i].new_ID) != str:\n",
    "        Table_BT.loc[i,\"new_ID\"] = past_ID\n",
    "    else:\n",
    "        past_ID = Table_BT.loc[i].new_ID\n",
    "Table_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_EMR[\"count_idx\"] = [0 for _ in range(len(Table_EMR))]\n",
    "Table_EMR.index = [Table_EMR[\"new_ID\"],Table_EMR[\"count_idx\"]]\n",
    "Table_EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_BT[\"Date_gap\"] = ['' for _ in range(len(Table_BT))]\n",
    "Table_BT.index = [Table_BT[\"new_ID\"],Table_BT[\"count_idx\"]]\n",
    "Table_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in Table_BT.index.get_level_values(0).unique().values:\n",
    "    if ID in Table_EMR.index.get_level_values(0):\n",
    "        Table_BT.loc[ID,\"Date_gap\"] = [int(((datetime.datetime.strptime(i, '%Y-%m-%d').timestamp()-datetime.datetime.strptime(str(Table_EMR.loc[ID][\"Date\"].values[0])[:10], '%Y-%m-%d').timestamp())/86400+30)//90)  for i in Table_BT.loc[ID][\"Date\"].values]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_BT[\"count_idx\"] = Table_BT[\"Date_gap\"].values\n",
    "Table_BT.index = [Table_BT[\"new_ID\"],Table_BT[\"count_idx\"]]\n",
    "Table_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_cat.to_excel('230913_concat_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([Table_EMR,Table_BT]).sort_index(level = 0).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_cat = pd.concat([Table_BT,Table_EMR])\n",
    "Table_cat = Table_cat.sort_index(level = 1).sort_index(level = 0)\n",
    "Table_cat = Table_cat.iloc[2:] \n",
    "Table_cat = Table_cat.groupby(level = 0,axis = 1).first()\n",
    "Table_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_Date = np.array([i.timestamp() for i in Table_cat[\"Date\"]])\n",
    "Table_Date = (Table_Date-np.mean(Table_Date))/ np.std(Table_Date)\n",
    "Table_Date = pd.DataFrame(Table_Date, index = Table_cat.index)\n",
    "Table_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(Table_cat+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_cat.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_cat = Table_cat[Var_con + Var_bi]\n",
    "Table_cat = np.log2(Table_cat.astype(float)+1)\n",
    "for var in Var_con:\n",
    "    Table_cat[var] = (Table_cat[var]-Table_cat[var].dropna().mean())/Table_cat[var].dropna().std()\n",
    "    # Table_X[var] = Table_X[var] - Table_X[var].dropna().min() + eps\n",
    "# Table_L_Con = Table_X[Var_Con].max(axis = 0)\n",
    "\n",
    "Table_X_norm = Table_cat.copy()\n",
    "Table_X_mask = Table_cat.isna()\n",
    "Table_X_norm[Var_con] = Table_X_norm[Var_con].fillna(0)#(eps)\n",
    "Table_X_norm[Var_bi] = Table_X_norm[Var_bi].fillna(0)\n",
    "Table_val_Weight = (Table_X_mask.shape[0] / (Table_X_mask.shape[0]-Table_X_mask.sum(axis = 0)))\n",
    "\n",
    "T_X_bi = Table_X_norm[Var_bi]\n",
    "T_X_con = Table_X_norm[Var_con]\n",
    "T_M_bi = Table_X_mask[Var_bi]\n",
    "T_M_con = Table_X_mask[Var_con]\n",
    "T_W_bi = Table_val_Weight[Var_bi]\n",
    "T_W_con = Table_val_Weight[Var_con]\n",
    "T_Date = Table_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output : ID_train & ID_test\n",
    "ID_all = Table_cat.index.get_level_values(level=0).unique().values\n",
    "np.random.shuffle(ID_all)\n",
    "\n",
    "Train_ratio = 0.8\n",
    "ID_train = ID_all[:int(len(ID_all)*Train_ratio)]\n",
    "ID_test = ID_all[int(len(ID_all)*Train_ratio):]\n",
    "print(ID_all.shape,ID_train.shape, ID_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IDs\n",
    "pd.DataFrame(ID_train).to_excel(\"Train_ID_set.xlsx\")\n",
    "pd.DataFrame(ID_test).to_excel(\"Test_ID_set.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class VAE_Dataset(Dataset):\n",
    "    def __init__(self, X1, X2, M1, M2, T_Date, ID_list):\n",
    "\n",
    "        self.x1 = torch.from_numpy(X1.loc[ID_list].values)\n",
    "        self.m1 = torch.from_numpy(M1.loc[ID_list].values)\n",
    "        \n",
    "        self.x2 = torch.from_numpy(X2.loc[ID_list].values)\n",
    "        self.m2 = torch.from_numpy(M2.loc[ID_list].values)\n",
    "\n",
    "        self.op_data = torch.from_numpy(T_Date.loc[ID_list].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x1[idx], self.x2[idx], self.m1[idx], self.m2[idx], self.op_data[idx]\n",
    "    \n",
    "class VAE_sampler():\n",
    "    def __init__(self, rand_p = 0.05):\n",
    "        self.rand_p = rand_p\n",
    "    def VAE_rand_fn(self, batch):\n",
    "        x1,x2,m1,m2,op_data = zip(*batch)\n",
    "        r_mask_ratio = self.rand_p\n",
    "        eps = 1e-6\n",
    "        \n",
    "        x1 = torch.stack(x1).float()\n",
    "        x2 = torch.stack(x2).float()\n",
    "        m1 = torch.stack(m1).bool()\n",
    "        m2 = torch.stack(m2).bool()\n",
    "        op_data = torch.stack(op_data).float()\n",
    "        \n",
    "        \n",
    "        r1 = (torch.rand_like(x1)<r_mask_ratio).bool()\n",
    "        x1 = x1 * (~r1) + 0 * r1\n",
    "        m1 = m1 + r1\n",
    "        \n",
    "        r2 = (torch.rand_like(x2)<r_mask_ratio).bool()\n",
    "        x2 = x2 * (~r2) + 0 * r2\n",
    "        m2 = m2 + r2\n",
    "        \n",
    "        return x1, x2, m1, m2, op_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_sampler_use = VAE_sampler(0.1)\n",
    "train_dataloader = DataLoader(VAE_Dataset(T_X_bi,T_X_con,T_M_bi,T_M_con,T_Date,ID_train),\n",
    "                              batch_size=64, shuffle=True, collate_fn=VAE_sampler_use.VAE_rand_fn)\n",
    "test_dataloader = DataLoader(VAE_Dataset(T_X_bi,T_X_con,T_M_bi,T_M_con,T_Date,ID_test),\n",
    "                             batch_size=64, shuffle=True, collate_fn=VAE_sampler_use.VAE_rand_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_VAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_sizes = [0,0],\n",
    "                 layer_size = [[],[]],\n",
    "                 norm_mth = 'Layer_norm'):\n",
    "        \n",
    "        super(BC_VAE,self).__init__()\n",
    "        input_size = input_sizes[0] + input_sizes[1]\n",
    "        Data_model_layer = layer_size\n",
    "        Data_model_layer[0][0] = input_size*2+1\n",
    "        Data_model_layer[1][0] += input_size+1\n",
    "        Data_model_layer[1][-1] = input_size\n",
    "\n",
    "        self.Encoder = Test_Encoder(Data_model_layer[0], normalize = norm_mth)\n",
    "        self.Decoder = Test_Decoder(Data_model_layer[1], normalize = norm_mth, output_sizes = input_sizes)\n",
    "        self.Data_model_layer = Data_model_layer\n",
    "        \n",
    "    def forward(self,inputs): # x,mask,y\n",
    "        x1,x2,m1,m2,op_date = inputs\n",
    "        mask_with_date = torch.concat([m1,m2,op_date+torch.randn_like(op_date)*0.5],axis = -1)\n",
    "        x = torch.concat([x1,x2], axis= -1)\n",
    "        m_s, log_v_s, sample_s = self.Encoder(x,mask_with_date)\n",
    "        outputs = self.Decoder(sample_s,mask_with_date)\n",
    "        return outputs, m_s, log_v_s\n",
    "    \n",
    "class Test_Encoder(nn.Module):\n",
    "    def __init__(self, layer_size = [32,128,24,16], normalize = False):\n",
    "        super(Test_Encoder,self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.Encoder = VAE_Encoder_masklayer(layer_size, normalize = normalize)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        return self.Encoder(torch.concat([x,mask],axis = -1))\n",
    "    \n",
    "class Test_Decoder(nn.Module):\n",
    "    def __init__(self, layer_size = [32,128,24,16], normalize = False, output_sizes = []):\n",
    "        super(Test_Decoder,self).__init__()\n",
    "        self.output_sizes = output_sizes\n",
    "        self.layer_size = layer_size\n",
    "        \n",
    "        self.Merged_Decoder = Merged_Decoder(layer_size, normalize = normalize,output_sizes = output_sizes)\n",
    "    def forward(self,sampled,mask):\n",
    "        return self.Merged_Decoder(torch.concat([sampled,mask],axis = -1))\n",
    "    \n",
    "class VAE_Encoder_masklayer(nn.Module):\n",
    "    def __init__(self, layer_size = [32,128,24,16], normalize = False):\n",
    "        super(VAE_Encoder_masklayer,self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.depth = len(layer_size)-1\n",
    "        self.blocks = nn.ModuleDict()\n",
    "        for n in range(self.depth-1):\n",
    "            self.blocks.__setitem__(str(n),FLblock(layer_size[n], layer_size[n+1], normalize = normalize, drop_out = 0.1))\n",
    "        n = self.depth-1\n",
    "        self.m_block = FLblock(layer_size[n], layer_size[n+1], activ_layer = False, normalize = False)\n",
    "        self.v_block = FLblock(layer_size[n], layer_size[n+1], activ_layer = False, normalize = False)\n",
    "    def forward(self,x):\n",
    "        for n in range(self.depth-1):\n",
    "            x = self.blocks[str(n)](x)\n",
    "        x_m = self.m_block(x)\n",
    "        x_log_v = self.v_block(x)\n",
    "        return x_m, x_log_v, torch.randn_like(x_m).mul(torch.exp(0.5*x_log_v)).add_(x_m)\n",
    "\n",
    "class Merged_Decoder(nn.Module):\n",
    "    def __init__(self, layer_size = [32,128,24,16], normalize = False, drop_out = 0.1, output_sizes = [0,0]):\n",
    "        super().__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.depth = len(layer_size)-1\n",
    "        self.blocks = nn.ModuleDict()\n",
    "        for n in range(self.depth-1):\n",
    "            self.blocks.__setitem__(str(n),FLblock(layer_size[n], layer_size[n+1], normalize = normalize, drop_out = drop_out))#, bias = False))\n",
    "        n = self.depth-1\n",
    "        # bi \n",
    "        self.Bi_p_block = FLblock(layer_size[n], output_sizes[0], activ_layer = 'sigmoid', normalize = False, clamp = 1e4)\n",
    "        # con : all GGM\n",
    "        self.Con_mse_block = FLblock(layer_size[n], output_sizes[1], activ_layer = False, normalize = False, clamp = True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for n in range(self.depth-1):\n",
    "            x = self.blocks[str(n)](x)\n",
    "        output = {}\n",
    "        output['Continuous'] = (self.Con_mse_block(x))                \n",
    "        output['Binary'] = (self.Bi_p_block(x))\n",
    "\n",
    "        return output\n",
    "    \n",
    "class FLblock(nn.Module):\n",
    "    def __init__(self, channel_in,channel_out, activ_layer = 'relu',drop_out = False, normalize = False, clamp = False, bias = True):\n",
    "        super().__init__()\n",
    "        self.activ_layer = activ_layer\n",
    "        self.Linear_in = nn.Linear(channel_in,channel_out, bias=bias)\n",
    "        self.drop_out = drop_out\n",
    "        self.normalize = normalize\n",
    "        self.clamp = clamp\n",
    "        if normalize == 'Layer_norm':\n",
    "            self.Norm = nn.LayerNorm([channel_out])\n",
    "        elif normalize == 'Batch_norm':\n",
    "           self.Norm = nn.BatchNorm1d([channel_out])\n",
    "    def forward(self, x):\n",
    "        block_1 = self.Linear_in(x)\n",
    "        if self.clamp is not False:\n",
    "            if self.clamp is True:\n",
    "                block_1 = torch.clamp(block_1,max = 1e6, min = -1e6)\n",
    "            else:\n",
    "                block_1 = torch.clamp(block_1,max = self.clamp, min = -1*self.clamp)\n",
    "                \n",
    "        if self.activ_layer is not False: # False, Relu, Sigmoid, Exp\n",
    "            if self.activ_layer == 'sigmoid':\n",
    "                block_1 = nn.Sigmoid()(block_1)\n",
    "            elif self.activ_layer == 'exp':\n",
    "                block_1 = torch.exp(block_1)\n",
    "            elif self.activ_layer == 'softmax':\n",
    "                block_1 = nn.Softmax(dim = -1)(block_1)\n",
    "            elif self.activ_layer == 'relu':\n",
    "                block_1 = nn.ReLU()(block_1)\n",
    "            elif self.activ_layer == 'softplus':\n",
    "                block_1 = nn.Softplus()(block_1)\n",
    "        if self.normalize is not False:\n",
    "            block_1 = self.Norm(block_1)\n",
    "        if self.drop_out is not False:\n",
    "            block_1 = nn.Dropout(p = self.drop_out)(block_1)\n",
    "        return block_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_model(model,optimizer,dataloader,device,train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    epoch_loss = {\n",
    "        \"KLD\" : 0,\n",
    "        \"Continuous\" : 0,\n",
    "        \"Binary\" : 0,\n",
    "    }\n",
    "    i_num = 0\n",
    "    for x1,x2,m1,m2,op_date in dataloader:\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            i_num += x1.shape[0]\n",
    "            # model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            m1 = m1.to(device)\n",
    "            m2 = m2.to(device)\n",
    "            op_date = op_date.to(device)\n",
    "            Model_out_s, m_s, log_v_s = model([x1,x2,m1,m2,op_date])\n",
    "            \n",
    "            loss_KLD = -0.5 * torch.sum(1 + log_v_s - m_s.pow(2) - log_v_s.exp(), axis = -1).mean()\n",
    "            \n",
    "            # GGM\n",
    "            dist = 'Continuous'\n",
    "            Cont_y = Model_out_s[dist]\n",
    "            Cont_x = x2\n",
    "            Cont_mask = m2\n",
    "            Cont_Weight = T_W_con        \n",
    "            case_Cont = (-1)*(Cont_x-Cont_y).pow(2)\n",
    "                \n",
    "            loss_Continuous = (-1)*torch.sum(torch.mul(torch.sum(torch.mul((Cont_mask<0.1).type(torch.float64),case_Cont),axis = 0)\n",
    "                                            /x2.shape[0],Cont_Weight))\n",
    "            \n",
    "            # Ber\n",
    "            dist = 'Binary'\n",
    "            \n",
    "            Ber_p = Model_out_s[dist]\n",
    "                \n",
    "            Ber_x = x1\n",
    "            Ber_mask = m1\n",
    "            Ber_Weight = T_W_bi\n",
    "            \n",
    "            case_Ber = (torch.mul((Ber_x<0.5).float(),torch.log((1-Ber_p)*0.9+0.05))\n",
    "                        +torch.mul((Ber_x>=0.5).float(),torch.log(Ber_p)*0.9+0.05))\n",
    "                \n",
    "            loss_Binary = (-1)*torch.sum(torch.mul(torch.sum(torch.mul((Ber_mask<0.1).float(),case_Ber),axis = 0)\n",
    "                                            /x1.shape[0],Ber_Weight))\n",
    "            \n",
    "            # loss = (loss_Continuous*10 + loss_Binary)*5\n",
    "            loss = loss_KLD + (loss_Continuous + loss_Binary)*5\n",
    "            epoch_loss[\"KLD\"] += loss_KLD * x1.shape[0]\n",
    "            epoch_loss[\"Continuous\"] += loss_Continuous * x1.shape[0]\n",
    "            epoch_loss[\"Binary\"] += loss_Binary * x1.shape[0]\n",
    "            if train:\n",
    "                try:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                except:\n",
    "                    print(Ber_x,case_Ber)\n",
    "    epoch_loss[\"KLD\"] /= i_num\n",
    "    epoch_loss[\"Continuous\"] /= i_num\n",
    "    epoch_loss[\"Binary\"] /= i_num\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_count = 100\n",
    "layer_data = [[1, 1024, 512, latent_count], [latent_count, 512, 1024, 1]]\n",
    "device = \"cuda:1\"\n",
    "model = BC_VAE(input_sizes = [T_X_bi.shape[1],T_X_con.shape[1]],\n",
    "                 layer_size = layer_data,\n",
    "                 norm_mth = 'Layer_norm').to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_W_con = torch.tensor(T_W_con.values, device = device).requires_grad_(False)\n",
    "T_W_bi = torch.tensor(T_W_bi.values, device = device).requires_grad_(False)\n",
    "T_W_con = (T_W_con+1).log2()\n",
    "# T_W_con[:10] = 4*T_W_con[:10]\n",
    "T_W_bi = (T_W_bi+1).log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "timer_use = tqdm.tqdm(range(epochs))\n",
    "losses = []\n",
    "\n",
    "for epoch in timer_use:\n",
    "    loss = Run_model(model,optimizer,train_dataloader,device,True)\n",
    "\n",
    "    t = f\"epoch: {epoch:04d} \" + \\\n",
    "        f\"loss: KL {loss['KLD']:.3f} C {loss['Continuous']:.3f} B {loss['Binary']:.3f} |\"\n",
    "        \n",
    "    loss = Run_model(model,optimizer,test_dataloader,device,False)\n",
    "\n",
    "    t += f\"validate : KL {loss['KLD']:.3f} C {loss['Continuous']:.3f} B {loss['Binary']:.3f}\"\n",
    "        \n",
    "    timer_use.set_description(t)\n",
    "\n",
    "    \n",
    "    losses.append(loss)\n",
    "torch.save(model, f\"{model_dir}/230913_VAE_beta_5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_1 = 2\n",
    "sampled = np.array([])\n",
    "sampled_real = np.array([])\n",
    "sampled_mask = np.array([])\n",
    "m_ss, log_v_ss = [], []\n",
    "\n",
    "for x1,x2,m1,m2,op_date in test_dataloader:\n",
    "    model.zero_grad()\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    m1 = m1.to(device)\n",
    "    m2 = m2.to(device)\n",
    "    op_date = op_date.to(device)\n",
    "    for _ in range(rep_1):\n",
    "        dist = 'Continuous'\n",
    "        Model_out_s, m_s, log_v_s = model([x1,x2,m1,m2,op_date])\n",
    "        GGM_x =  x2.detach().cpu().numpy()\n",
    "        GGM_mask = m2.detach().cpu().numpy()\n",
    "        data_sample = Model_out_s[dist].detach().cpu().numpy()\n",
    "        sampled_real = np.concatenate([sampled_real,GGM_x],axis = 0) if len(sampled_real) else GGM_x\n",
    "        sampled = np.concatenate([sampled,data_sample],axis = 0) if len(sampled) else data_sample\n",
    "        sampled_mask = np.concatenate([sampled_mask,GGM_mask],axis = 0) if len(sampled_mask) else GGM_mask\n",
    "        m_ss = np.concatenate([m_ss,m_s.detach().cpu().numpy()],axis = 0) if len(m_ss) else m_s.detach().cpu().numpy()\n",
    "        log_v_ss = np.concatenate([log_v_ss,log_v_s.detach().cpu().numpy()],axis = 0) if len(log_v_ss) else log_v_s.detach().cpu().numpy()\n",
    "    \n",
    "    # # Ber\n",
    "    # dist = 'Binary'\n",
    "    \n",
    "    # Ber_p = Model_out_s[dist]\n",
    "        \n",
    "    # Ber_x = x1\n",
    "    # Ber_mask = m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_i = 0\n",
    "plt.figure(figsize=(5,5))\n",
    "for n, i in enumerate(Var_con):\n",
    "    idx_i +=1\n",
    "    plt.subplot(5,5,idx_i)\n",
    "    plt.scatter(sampled_real[sampled_mask[:,n] == 0,n],sampled[sampled_mask[:,n] == 0,n],s = 1, c = 'k')\n",
    "    plt.xticks([0,np.max(sampled_real[sampled_mask[:,n] == 0,n])],['',''])\n",
    "    plt.yticks([0,np.max(sampled_real[sampled_mask[:,n] == 0,n])],['',''])\n",
    "    plt.xlim(np.min(sampled_real[sampled_mask[:,n] == 0,n])-0.5,np.max(sampled_real[sampled_mask[:,n] == 0,n]+0.5))\n",
    "    plt.ylim(np.min(sampled_real[sampled_mask[:,n] == 0,n])-0.5,np.max(sampled_real[sampled_mask[:,n] == 0,n]+0.5))\n",
    "    # plt.title(i)\n",
    "    if idx_i == 25:\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(5,5))\n",
    "        idx_i = 0\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "idx_i = 0\n",
    "for i in range(100):\n",
    "    idx_i+=1\n",
    "    plt.subplot(5,5,idx_i)\n",
    "    plt.scatter(m_ss[:,i],np.exp(0.5*log_v_ss[:,i]), s = 1)\n",
    "    plt.plot([-3,3],[0,0],'k')\n",
    "    plt.plot([-3,3],[1,1],'k-')\n",
    "    plt.plot([0,0],[-1,3],'k')\n",
    "    plt.xticks([-2,0,2])\n",
    "    plt.yticks([0,0.5,1])\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-0.1,1.51)\n",
    "    if idx_i == 25:\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(5,5))\n",
    "        idx_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_X_norm.loc[\"P_001469\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_cat.index = [Table_cat.index.get_level_values(0),Table_cat.index.get_level_values(1)//4]\n",
    "Table_cat = Table_cat.groupby(level = [0,1]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_cat = Table_cat.groupby(level = [0,1]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Table_X_norm.index = [Table_X_norm.index.get_level_values(0),Table_X_norm.index.get_level_values(1)//4]\n",
    "Table_X_norm = Table_X_norm.groupby(level = [0,1]).first()\n",
    "Table_X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Table_X_mask.index = [Table_X_mask.index.get_level_values(0),Table_X_mask.index.get_level_values(1)//4]\n",
    "Table_X_mask = Table_X_mask.groupby(level = [0,1]).first()\n",
    "Table_X_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_Date.index = [Table_Date.index.get_level_values(0),Table_Date.index.get_level_values(1)//4]\n",
    "Table_Date = Table_Date.groupby(level = [0,1]).first()\n",
    "Table_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_in_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idx_con = [n for n,i in enumerate(Table_cat.columns) if i in Var_con]\n",
    "Idx_bi = [n for n,i in enumerate(Table_cat.columns) if i in Var_bi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_Encoder = model.Encoder\n",
    "ID_set = {}\n",
    "ID_set['train'] = ID_train\n",
    "ID_set['test'] = ID_test\n",
    "IDs = np.unique(Table_cat.index.get_level_values(level = 0))\n",
    "ID_list = []\n",
    "ID_year_list = []\n",
    "TF_in_data = np.array([])\n",
    "TF_in_std = np.array([])\n",
    "TF_in_mask = np.array([])\n",
    "tqdm_epoch = tqdm.tqdm(IDs)\n",
    "using_z = [i for i in range(100)]\n",
    "for ID in tqdm_epoch:\n",
    "    TF_in_data_sub = torch.zeros((1,10,len(using_z)), device = device).float()\n",
    "    TF_in_std_sub = torch.ones((1,10,len(using_z)), device = device).float()\n",
    "    TF_in_mask_sub = torch.ones((1,10,1), device = device).float()\n",
    "    Table_ID_idx = np.where(Table_cat.index.get_level_values(level = 0) == ID)[0]\n",
    "    Table_sub = Table_cat.loc[ID]\n",
    "    Table_in_date = torch.tensor(Table_sub[\"Test_date_normed\"].loc[Table_sub.index < 9].values,device = device).float().unsqueeze(1)\n",
    "    ID_year = []\n",
    "    ID_idx  = []\n",
    "    for n, i in enumerate(Table_sub.index):\n",
    "        if i < 9: # (-1 ~ 9year의 데이터만 사용할 예정)\n",
    "            ID_year += [i]\n",
    "            ID_idx += [Table_ID_idx[n]]\n",
    "    if len(ID_year)>0:\n",
    "        ID_year_sub= []\n",
    "        Table_in_norm = torch.tensor(np.array(Table_X_norm.loc[ID].loc[ID_year])[:,Idx_bi+Idx_con],device = device).float()\n",
    "        Table_in_mask = torch.tensor(np.array(Table_X_mask.loc[ID].loc[ID_year])[:,Idx_bi+Idx_con],device = device).float()\n",
    "        Table_in_mask = torch.concat([Table_in_mask,Table_in_date+torch.randn_like(Table_in_date)*0.5],axis = -1)\n",
    "        m_s, log_v_s, sample_s = VAE_Encoder(Table_in_norm,Table_in_mask)\n",
    "        for i in range(m_s.shape[0]):\n",
    "            if ID_year[i] <-1:\n",
    "                if -1 in ID_year_sub:\n",
    "                    continue\n",
    "                else:\n",
    "                    ID_year[i] = -1\n",
    "                    ID_year_sub += [-1]\n",
    "            else:\n",
    "                ID_year_sub += [ID_year[i]]\n",
    "            TF_in_data_sub[0,(ID_year[i]+1),using_z] = m_s[i,using_z]\n",
    "            TF_in_std_sub[0,(ID_year[i]+1),using_z] = (log_v_s[i,using_z]/2).exp()\n",
    "            TF_in_mask_sub[0,(ID_year[i]+1)] = 0\n",
    "        TF_in_data = TF_in_data_sub if TF_in_data.shape[0] ==0 else torch.concat([TF_in_data,TF_in_data_sub],axis = 0)\n",
    "        TF_in_std = TF_in_std_sub if TF_in_std.shape[0] ==0 else torch.concat([TF_in_std,TF_in_std_sub],axis = 0)\n",
    "        TF_in_mask = TF_in_mask_sub if TF_in_mask.shape[0] ==0 else torch.concat([TF_in_mask,TF_in_mask_sub],axis = 0)\n",
    "        ID_list += [ID]\n",
    "        ID_year_list += [ID_year_sub]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_in_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_X_norm[Var_bi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TF_in_data_all.pickle', 'wb') as f:\n",
    "    pickle.dump([TF_in_data, TF_in_std, TF_in_mask, ID_list, ID_year_list], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_X_norm.loc[[ID]].iloc[Table_X_norm.loc[[ID]].index.get_level_values(level = 1)<9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env_yoo_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
